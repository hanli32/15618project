{
  "name": "Parallel Linear Solver using PCG",
  "tagline": "Team Members: Bo Feng, Han Li",
  "body": "# Summary\r\nWe are going to implement a parallel linear solver using preconditioned conjugate gradient algorithm on GPU.\r\n\r\n# Background\r\nThe Poisson equation arises in many applications in computational fluid dynamics, electrostatics, magnetostatics, image processing, etc. Numerical solution of the Poisson equation, through finite element or finite difference discretization, leads to large sparse linear systems usually solved by iterative methods instead of direct methods (Gaussian elimination or Cholesky factorization). The conjugate gradient (CG) algorithm is one of the best known iterative methods for solving linear systems with symmetric, positive definite matrix.   \r\n\r\nThe CG method can be easily adapted for linear systems with symmetric, semi-definite positive matrix. With a suitable preconditioner, the performance can be dramatically increased. The preconditioned conjugate gradient (PCG) has proven its efficiency and robustness in a wide range of applications. Our goal is to develop a parallel linear solver with an efficient PCG algorithm for the GPU architecture. \r\n\r\n# The Challenge\r\nFirstly since we are going to process large but sparse matrix, it is necessary to use a special data structure. It is common to keep only non-zero elements. The challenge here is how to choose a data structure that can not only save space but also take advantage of data locality. Also, with the use of special data structure now each row has different number of elements, as a result we have to design an efficient schedule method but not simply assign one row of matrix to a thread, which may lead to uneven wordload.  \r\n\r\nSecondly since the preconditioned conjugate gradient algorithm is an iterative method, it is impossible to parallel different iterations. In each iteration there are several variable updates and some variables are dependent to other variables, for example in order to update variable z we must have a new version of variable a. Our challenge here is how to reduce the time of synchronization.  \r\n\r\nIn each iteration there two important operations, the first one is inner product and the second one is matrix-vector mulplication. For inner product we plan to use the prefix sum algorithm. The challenge here is how to implement an efficient prefix sum when the size of vector is not the power of 2. For matrix-vector mulplication we can parallel on matrixâ€™s rows and the challenge is how to decide the number of elements each CUDA thread needs to process and the number of threads per CUDA block to better use of GPU resources like shared memory.   \r\n \r\n# Resources\r\nThere are some efficient methods to do sparse matrix-vector mulplication in this paper[1]. Cuda also has a document[2] about how to efficiently parallel prefix sum.  \r\n\r\nWe are going to use GPUs in GHC machines.\r\n\r\n# Goals and Deliverables\r\nWe are going to implement a serial version and parallel version of the preconditioned conjugate gradient algorithm. We plan to speed up the parallel version at least 10x compared to the serial version. We are going to test our implementation on the The University of Florida Sparse Matrix Collection[3] and we hope the performance of our implementation can tie the state-of-the-art library like SuiteSpace.   \r\n\r\nWe are going to demonstrate our project by plotting speedup and execution time graphs compared to the serial version as well as the state-of-the-art library.   \r\n\r\n# Platform Choice\r\nWe are going to use CUDA framework on both GHC machines and latedays clusters in order to have a low-level control of GPUs and maximize parallelism speedup.\r\n\r\n# Schedule\r\n### April 3rd - Apri 9th\r\n* Do more background research.\r\n* Read CUDA documents and get familiar with CUDA optimization.\r\n\r\n### April 10th - April 16th\r\n* Begin serial implementation of the preconditioned conjugate gradient algorithm.\r\n\r\n### April 17th - April 23rd\r\n* Finish serial implementation.\r\n* Begin parallel the preconditioned conjugate gradient algorithm.\r\n\r\n### April 24th - April 30th\r\n* Finish a correct parallel implementation.\r\n* Optimize parallel implementation to achieve objective speedup.\r\n\r\n### May 1st - May 7th\r\n* Prepare demos.\r\n* Analyze final results.\r\n* Finish final report.\r\n\r\n# references\r\n1. Bell, Nathan, and Michael Garland. Efficient sparse matrix-vector multiplication on CUDA. Vol. 2. No. 5. Nvidia Technical Report NVR-2008-004, Nvidia Corporation, 2008.\r\n2. Harris, Mark, Shubhabrata Sengupta, and John D. Owens. \"Parallel prefix sum (scan) with CUDA.\" GPU gems 3.39 (2007): 851-876.\r\n3. http://www.cise.ufl.edu/research/sparse/matrices/\r\n\r\n\r\n",
  "note": "Don't delete this file! It's used internally to help with page regeneration."
}